# MLP-Mixer : An all-MLP Architecture for Vision(2021-06)

## 0 Abstract

> When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models.
> 

large datasetsì— modern regularization techniqueë¡œ trainì„ í•˜ë©´MLP-mixerëŠ” ê²½ìŸë ¥ìˆëŠ” performanceë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
image classificationì—ì„œ, ê·¸ë¦¬ê³  pre-trainingê³¼ inference costì—ì„œ sotaì— ê²½ìŸë ¥ ìˆëŠ” ëª¨ìŠµì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

// image classificationì™¸ì— ë‹¤ë¥¸ taskì—ì„œì˜ ì„±ëŠ¥ì€ ì–´ë–¨ê¹Œ ê¶ê¸ˆí•©ë‹ˆë‹¤.

## 1 Introduction

> Mixer relies only on basic matrix multiplication routines, changes to data layout (reshapes and transpositions), and scalar nonlinearities.
> 

ë¯¹ìŠ¤ëŠ” matmulê³¼ data layout ë³€ê²½(reshape and tranpositions), scalar ë¹„ì„ í˜•ì„±ì—ë§Œ ì˜ì¡´í•©ë‹ˆë‹¤.

> In the extreme case, our architecture can be seen as a very special CNN, which uses 1Ã—1 convolutions for channel mixing, and single-channel depth-wise convolutions of a full receptive field and parameter sharing for token mixing.
> 

ê·¹ë‹¨ì ì¸ ê²½ìš°, ìš°ë¦¬ ì•„í‚¤í…ì²˜ëŠ” íŠ¹ë³„í•œ CNNìœ¼ë¡œ ë³´ì¼ìˆ˜ ìˆìŠµë‹ˆë‹¤.
(1x1 convë¥¼ ì²´ë„ ë¯¹ì‹±ì— í™œìš©í•˜ê³ , í† í° ë¯¹ì‹±ì— ë‹¨ì¼ ì²´ë„ íŒŒë¼ë¯¸í„° sharing convë¥¼ ì‚¬ìš©í•˜ëŠ” íŠ¹ë³„í•œ CNNìœ¼ë¡œ)

## 2 Mixer Architecture

![Untitled](./mlp-mixer/Untitled.png)

![Untitled](./mlp-mixer/Untitled1.png)

> Therefore, the computational complexity of the network is linear in the number of input patches, unlike ViT whose complexity is quadratic.
> 

ê·¸ëŸ¬ë¯€ë¡œ, computational complexityëŠ” patchesì— ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•©ë‹ˆë‹¤(vitê°€ ^2ìœ¼ë¡œ ì¦ê°€í•˜ëŠ”ê²ƒê³¼ëŠ” ë‹¤ë¥´ê²Œ)

> the overall complexity is linear in the number of pixels in the image, as for a typical CNN.
> 

ì „ì²´ì ì¸ complexityëŠ” CNNê³¼ ê°™ì´ ì´ë¯¸ì§€ ì „ì²´ì˜ í”½ì…€ê°œìˆ˜ì— ë¹„ë¡€í•˜ì—¬ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•©ë‹ˆë‹¤.

> However, in separable convolutions, a different convolutional kernel is applied to each channel unlike the token-mixing MLPs in Mixer that share the same kernel (of full receptive field) for all of the channels.
> 

í•˜ì§€ë§Œ, channel ë§ˆë‹¤ ê°ê¸° ë‹¤ë¥¸ kernelì´ applyë˜ëŠ” separable convì™€ëŠ” ë‹¤ë¥´ê²Œ token-mixing MLPsëŠ” ëª¨ë“  ì²´ë„ì—ì„œ ê°™ì€ kernelì„ ê°€ì§‘ë‹ˆë‹¤.

> Mixer does not use position embeddings because the token-mixing MLPs are sensitive to the order of the input tokens.
> 

ë¯¹ì„œëŠ” PEë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì™œëƒí•˜ë©´ token-mixingì€ input tokenì˜ ìˆœì„œì— sensitiveí•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

// stack ìˆœì„œì—ì„œ position infoë¥¼ ì–»ëŠ”ë‹¤ëŠ” ì˜ë¯¸ë¡œ ìƒê°ë©ë‹ˆë‹¤.

## 3 Experiments

> We are interested in three primary quantities: (1) Accuracy on the downstream task; (2) Total computational cost of pre-training, which is important when training the model from scratch on the upstream dataset; (3) Test-time throughput, which is important to the practitioner.
> 

ìš°ë¦¬ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ìˆ˜ì¹˜ì— ê´€ì‹¬ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
(1) downstream taskì—ì„œì˜ acc
(2) upstream taskì—ì„œ scratchë¶€í„° ì‹œì‘í•˜ëŠ” pretrain ê³„ì‚°ë¹„ìš©
(3) ì‹¤ë¬´ì—ì„œ ì¤‘ìš”í•œ test ì‹œê°„ ì²˜ë¦¬

> HaloNets are attention-based models that use a ResNet-like structure with local selfattention layers instead of 3Ã—3 convolutions [51].
Big Transfer (BiT) [22] models are ResNets optimized for transfer learning. 
NFNets [7] are normalizer-free ResNets with several optimizations for ImageNet classification.
MPL is pre-trained at very large-scale on JFT-300M images, using meta-pseudo labelling from ImageNet instead of the original labels.
ALIGN pre-train image encoder and language encoder on noisy web image text pairs in a contrastive way.
> 

### 3.1 Main results

![Untitled](./mlp-mixer/Untitled2.png)

> When the size of the upstream dataset increases, Mixerâ€™s performance improves significantly.
> 

upstreamì˜ datasetì˜ ì‚¬ì´ì¦ˆê°€ ì¦ê°€í• ìˆ˜ë¡,
Mixerâ€™sì˜ performanceê°€ significantlyí•˜ê²Œ ì¦ê°€í–ˆìŠµë‹ˆë‹¤.

### 3.2 The role of the model scale

![Untitled](./mlp-mixer/Untitled3.png)

### 3.3 The role of the pre-training dataset size

> Thus, every model is pre-trained for the same number of total steps.
> 

ëª¨ë“  ëª¨ë¸ì€ ë™ì¼í•œ stepìœ¼ë¡œ pretrain ëœ ê²°ê³¼ì…ë‹ˆë‹¤.

> When pre-trained on the smallest subset of JFT-300M, all Mixer models strongly overfit. BiT models also overfit, but to a lesser extent, possibly due to the strong inductive biases associated with the convolutions. As the dataset increases, the performance of both Mixer-L/32 and Mixer-L/16 grows faster than BiT; Mixer-L/16 keeps improving, while the BiT model plateaus.
> 

JFT-300Mì˜ ê°€ì¥ ì‘ì€ subsetì— ëŒ€í•´ ì‚¬ì „ í•™ìŠµëœ ê²½ìš°, ëª¨ë“  ë¯¹ì„œ ëª¨ë¸ì´ ê°•ë ¥í•˜ê²Œ overfití•©ë‹ˆë‹¤. BiT ëª¨ë¸ë„ overfití•˜ì§€ë§Œ, ì»¨ë³¼ë£¨ì…˜ê³¼ ê´€ë ¨ëœ ê°•ë ¥í•œ inductive biasìœ¼ë¡œ ì¸í•´ ê·¸ ì •ë„ê°€ ëœí•©ë‹ˆë‹¤. 
ë°ì´í„° ì„¸íŠ¸ê°€ ì¦ê°€í•¨ì— ë”°ë¼ Mixer-L/32ì™€ Mixer-L/16ì˜ ì„±ëŠ¥ì€ ëª¨ë‘ BiTë³´ë‹¤ ë¹ ë¥´ê²Œ ì¦ê°€í•˜ë©°, Mixer-L/16ì€ ê³„ì† ê°œì„ ë˜ëŠ” ë°˜ë©´ BiT ëª¨ë¸ì€ ì •ì²´(plateus)ë©ë‹ˆë‹¤.

// ì™œ BiT(conv ê³„ì—´)ì€ ì •ì²´(plateus) ë ê¹Œ?

> It appears that Mixer benefits from the growing dataset size even more than ViT. One could speculate and explain it again with the difference in inductive biases: self-attention layers in ViT lead to certain properties of the learned functions that are less compatible with the true underlying distribution than those discovered with Mixer architecture.
> 

MixerëŠ” ViTë³´ë‹¤ ë°ì´í„° ì„¸íŠ¸ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ ë” ë§ì€ ì´ì ì„ ì–»ëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. inductive biasì˜ ì°¨ì´ë¡œ ë‹¤ì‹œ í•œ ë²ˆ ì¶”ì¸¡í•˜ê³  ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ViTì˜ MSAsì€ í•™ìŠµëœ í•¨ìˆ˜ì˜ íŠ¹ì • ì†ì„±(certain properties)ì„ ì‹¤ì œ ê¸°ë³¸ ë¶„í¬ì™€ ëœ í˜¸í™˜ë˜ëŠ” Mixer ì•„í‚¤í…ì²˜ì—ì„œ ë°œê²¬ë˜ëŠ” ê²ƒë³´ë‹¤ ë” ë§ì´ ìœ ë„í•©ë‹ˆë‹¤.

// weak inductive bias(MLP)ì˜ ê²½ìš° huge datasetì—ì„œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•  ê²ƒ ì´ë¼ëŠ” ë§ë¡œ ìƒê°ë©ë‹ˆë‹¤.

### 3.4 Invariance to input permutations

![Untitled](./mlp-mixer/Untitled4.png)

> As could be expected, Mixer is invariant to the order of patches and pixels within the patches (the blue and green curves match perfectly). On the other hand, ResNetâ€™s strong inductive bias relies on a particular order of pixels within an image and its performance drops significantly when the patches are permuted. Remarkably, when globally permuting the pixels, Mixerâ€™s performance drops much less (âˆ¼45% drop) compared to the ResNet (âˆ¼75% drop).
> 

// global shufflingì€ mixerì˜ performanceë„ í¬ê²Œ í•˜ë½ì‹œì¼°ìŠµë‹ˆë‹¤.
convì— patchê°œë…ì„ ë„£ì€ê²ƒì€ inductive biasë¥¼ ì¤„ì—¬ì¤€ ê²ƒì´ 
ì•„ë‹ê¹Œìš”?

### 3.5 Visualization

![Untitled](./mlp-mixer/Untitled5.png)

> It is commonly observed that the first layers of CNNs tend to learn Gabor-like detectors that act on pixels in local regions of the image. In contrast, Mixer allows for global information exchange in the token-mixing MLPs, which begs the question whether it processes information in a similar fashion.
> 

ì¼ë°˜ì ìœ¼ë¡œ CNNì˜ ì²« ë²ˆì§¸ ë ˆì´ì–´ëŠ” ì´ë¯¸ì§€ì˜ ë¡œì»¬ ì˜ì—­ì— ìˆëŠ” í”½ì…€ì— ì‘ìš©í•˜ëŠ” ê°€ë²„ì™€ ìœ ì‚¬í•œ ê²€ì¶œê¸°ë¥¼ í•™ìŠµí•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. ì´ì™€ëŠ” ëŒ€ì¡°ì ìœ¼ë¡œ ë¯¹ì„œëŠ” í† í° í˜¼í•© MLPì—ì„œ ê¸€ë¡œë²Œ ì •ë³´ êµí™˜ì„ í—ˆìš©í•˜ë¯€ë¡œ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ëŠ”ì§€ ì˜ë¬¸ì´ ìƒê¹ë‹ˆë‹¤.

Recall that the token-mixing MLPs allow global communication between different spatial locations.

í† í° í˜¼í•© MLPë¥¼ ì‚¬ìš©í•˜ë©´ ì„œë¡œ ë‹¤ë¥¸ ê³µê°„ ìœ„ì¹˜ ê°„ì˜ ê¸€ë¡œë²Œ í†µì‹ ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì„ ê¸°ì–µí•˜ì„¸ìš”.

## 4 Related work

> Many recent works strive to design more effective architectures for vision. Srinivas et al. [42] replace 3Ã—3 convolutions in ResNets by self-attention layers. Ramachandran et al. [37], Tay et al. [47], Li et al. [26], and Bello [3] design networks with new attention-like mechanisms. Mixer can be seen as a step in an orthogonal direction, without reliance on locality bias and attention mechanisms.
[3]: Modeling long-range interactions without attention.(2021)
[26]: Involution: Inverting the inherence of convolution for visual recognition.(2021)
[37]: Stand-alone self-attention in vision models.(2019)
[47]: Synthesizer: Rethinking self-attention in transformer models. (2020)
> 

## 5 Conclusions

> On the theoretical side, we would like to understand the inductive biases hidden in these various features and eventually their role in generalization.
> 

## reference(DSBA)

IBì— ëŒ€í•´ ë””í…Œì¼í•˜ê²Œ ì„¤ëª…í•´ ì£¼ëŠ” ì˜ìƒì…ë‹ˆë‹¤.

MLPì—ì„œ Inductive Biasë¥¼ ì¤„ì—¬ì£¼ìëŠ” ë°œìƒì…ë‹ˆë‹¤.

<aside>
ğŸ’¡ Inductive Bias

Inductive BiasëŠ” â€œì¼ë°˜í™”ë¥¼ ìœ„í•´ ë¯¸ë¦¬ ê²½í–¥ì„±ì„ êµ¬ì¡°ì ìœ¼ë¡œ ê°•ì œâ€í•´ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.

in the absence of string i.b., a model can be equally attracted to several local minima on the loss surface; and the converged solution can be arbitarily affected by random variations like the initial state of the order of training examples. [[ref](https://arxiv.org/abs/2006.00555)]

// i.b.ê°€ ì—†ìœ¼ë©´ local minimaì— ìˆ˜ë ´í•˜ê²Œ ë©ë‹ˆë‹¤.

</aside>

<aside>
ğŸ’¡ No free lunch Theorems

There is no such model that fits all possible situation

// ìƒí™©ë§ˆë‹¤ ê°€ì¥ fití•œ ëª¨ë¸ì€ ë‹¤ë¦…ë‹ˆë‹¤.

</aside>

[[Paper Review] MLP-Mixer: An all-MLP Architecture for Vision](https://youtu.be/Y-isY31Thkw)